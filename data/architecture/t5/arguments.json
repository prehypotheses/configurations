{
  "MAX_LENGTH": 512,
  "TRAIN_BATCH_SIZE": 8,
  "VALID_BATCH_SIZE": 8,
  "TEST_BATCH_SIZE": 4,
  "EPOCHS": 2,
  "LEARNING_RATE": 0.0000165,
  "WEIGHT_DECAY": 0.01,
  "MAX_GRADIENT_NORM": 10,
  "N_TRAIN": null,
  "N_VALID": null,
  "N_TEST": null,
  "N_CPU": 8,
  "N_GPU": 1,
  "N_TRIALS": 2,
  "N_INSTANCES": null,
  "save_total_limit": 2,
  "early_stopping_patience": 3,
  "perturbation_interval": 2,
  "quantile_fraction": 0.25,
  "resample_probability": 0.25,
  "task": "ner",
  "pretrained_model_name": "google-t5/t5-small",
  "architecture": "t5",
  "scheduler": "ASHAScheduler",
  "seed": 5,
  "fraction": 0.005,
  "raw_": "data/special",
  "tokens_": "data/tokens",
  "experiment_name": "FEW",
  "experiment_tags": {
    "project": "custom token classification",
    "type": "natural language processing",
    "task": "token classification",
    "description": "The fine-tuning of pre-trained large language model architectures for token classification tasks."}
}